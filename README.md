## Лабораторные работы по дисциплине "Прикладные системы и фреймворки искусственного интеллекта"
### Выполнил студент группы М8О-412Б-22 Жаворонков Михаил

## Описание проделанной работы
В рамках выполнения 5 лабораторных работ, исследующих различные модели машинного обучения:
1. **KNN** (k-ближайших соседей)
2. **Линейные модели** (Линейная и Логистическая регрессии)
3. **Дерево решений**
4. **Случайный лес**
5. **Градиентный бустинг**

Для каждой модели проведены эксперименты на двух датасетах:
- **Классификация**: предсказание трудоустройства разработчиков (73k+ строк, 14+ признаков) (100+ признаков после обработки)
- **Регрессия**: предсказание питательной ценности продуктов (таблица пищевых характеристик)

Для каждой задачи выполнено 4 этапа тестирования:
1. Модель sklearn на базовом бейзлайне
2. Модель sklearn на новом бейзлайне
3. Самописная реализация на базовом бейзлайне
4. Самописная реализация на новом бейзлайне

## Методология

### Базовый бейзлайн:
- Минимальная предобработка (кодирование категориальных признаков)
- Разделение на train/test (67/33)
- Обучение модели со стандартными параметрами

### Новый бейзлайн:
- Стандартизация признаков (StandardScaler)
- Кросс-валидация (5 folds)
- Подбор гиперпараметров через GridSearchCV
- Для линейных моделей: удаление мультиколлинеарных признаков + регуляризация

## Результаты экспериментов

### Сводная таблица лучших результатов

#### Классификация (Accuracy / F1):
| Модель | Лучшая реализация | Accuracy | F1 | Бейзлайн |
|--------|------------------|----------|----|----------|
| KNN | sklearn | 0.84 | 0.83 | Новый |
| Логистическая регрессия | Самописная | 0.983 | 0.984 | Новый |
| Дерево решений | sklearn | 0.9668 | 0.9681 | Новый |
| Случайный лес | Самописная | 0.998 | 0.998 | Новый |
| Градиентный бустинг | sklearn | 0.9994 | 0.9995 | Новый |

#### Регрессия (MAE / R²):
| Модель | Лучшая реализация | MAE | R² | Бейзлайн |
|--------|------------------|-----|----|----------|
| KNN | Самописная | 22.13 | 0.84 | Базовый |
| Линейная регрессия | sklearn | 2.36 | 0.991 | Новый |
| Дерево решений | sklearn | 16.6 | 0.917 | Базовый |
| Случайный лес | Самописная | 10.97 | 0.946 | Новый |
| Градиентный бустинг | sklearn | 10.18 | 0.9648 | Новый |

## Анализ результатов

### Лучшие модели:
1. **Для классификации**:
   - **Градиентный бустинг (sklearn)** показал максимальный Accuracy (0.9994)
   - **Случайный лес (самописный)** также показал отличный результат (0.998)
   - **Логистическая регрессия (самописная)** продемонстрировала баланс точности и интерпретируемости (0.983)

2. **Для регрессии**:
   - **Градиентный бустинг (sklearn)** - наименьшая MAE (10.18)
   - **Случайный лес (самописный)** - лучший баланс метрик
   - **Линейная регрессия** показала неожиданно высокие результаты, что указывает на сильную линейную зависимость в данных

### Худшие модели:
1. **KNN** показал наихудшие результаты в классификации (0.64-0.84)
2. **Самописное дерево решений** в регрессии показало низкое R² (0.35-0.41)

### Влияние нового бейзлайна:
- **Положительный эффект**: Для большинства моделей новый бейзлайн улучшил результаты
- **Наибольшее улучшение**: KNN (классификация) +0.2 Accuracy
- **Минимальный эффект**: Логистическая регрессия (sklearn) - метрики не изменились
- **Отрицательный эффект**: Самописный KNN (регрессия) - метрики ухудшились

## Общий вывод по всем лабораторным работам

Проведённые исследования пяти моделей машинного обучения подтвердили ожидаемую закономерность: сложность алгоритма прямо влияет на качество предсказаний. Ансамблевые методы (Случайный лес и Градиентный бустинг) показали наилучшие результаты, особенно в задаче классификации (Accuracy до 0.999). Линейные модели отлично справились с регрессией благодаря чёткой линейной зависимости в данных, но потребовали удаления мультиколлинеарности. Древовидные модели склонны к переобучению, что удалось частично исправить контролем глубины и подбором гиперпараметров. Новый бейзлайн со стандартизацией и кросс-валидацией улучшил большинство моделей, особенно KNN (+0.2 Accuracy). Самописные реализации в трёх случаях превзошли sklearn-версии, что доказывает ценность глубокого понимания алгоритмов. Для подобных задач рекомендуется начинать с линейных моделей для регрессии и ансамблевых методов для классификации, обязательно применяя кросс-валидацию и борьбу с переобучением.
